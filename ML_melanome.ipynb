{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tiiySK4_5IV",
        "outputId": "fc29f0d2-fbb0-4cf1-9e6d-539b3b0047d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-sQ6SQ42z9l",
        "outputId": "4efff926-bae3-4420-bdcf-b35e70a633e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train weights: [3.838918960420746e-05, 0.0020920502092050207]\n",
            "validation weights: [0.0001529753709652746, 0.009433962264150943]\n",
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset, random_split, WeightedRandomSampler\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.optim as optim\n",
        "import os\n",
        "\n",
        "# Data augmentation and normalization for training\n",
        "# Just normalization for validation\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip()\n",
        "    ]),\n",
        "    'validation': transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomVerticalFlip()\n",
        "    ])\n",
        "} # Simplified version: removed data augmentation and normalization\n",
        "\n",
        "data_dir = os.path.join(\"drive\", \"MyDrive\", \"CVML_project\")\n",
        "class_counts = {}\n",
        "images_datasets = {}\n",
        "dataset_sizes = {}\n",
        "for x in ['train', 'validation']:\n",
        "    dataset_path = os.path.join(data_dir, x)\n",
        "    class_counts[x] = [len(os.listdir(os.path.join(dataset_path, y))) for y in [\"class0\", \"class1\"]]\n",
        "    image_datasets[x] = datasets.ImageFolder(dataset_path, data_transforms[x])\n",
        "    dataset_sizes[x] = len(image_datasets[x])\n",
        "\n",
        "# Calculate class frequencies and weights\n",
        "weighted_samplers = {}\n",
        "for x in ['train', 'validation']:\n",
        "    total_samples = sum(class_counts[x])\n",
        "    weight = [(1.0 / count) for count in class_counts[x]]\n",
        "    print(f\"{x} weights:\", weight)\n",
        "    weighted_samplers[x] = WeightedRandomSampler(weight, total_samples)\n",
        "\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=64,\n",
        "                                              sampler=weighted_samplers[x], num_workers=2) for x in ['train', 'validation']}\n",
        "\n",
        "# Use random_split to create datasets for training and validation\n",
        "# train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "soDs9h1M8YLr"
      },
      "outputs": [],
      "source": [
        "# Use a pre-trained ResNet18 model\n",
        "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "num_classes = 2\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Modify the final fully connected layer to match the number of classes\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QC4xVg1zzYOg",
        "outputId": "82ab9579-dafa-4fe0-9bd3-4d57d5f551b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/4\n",
            "----------\n",
            "train Loss: 0.0080 Acc: 0.9952\n",
            "validation Loss: 0.0000 Acc: 1.0000\n",
            "\n",
            "Epoch 1/4\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 1.0000\n",
            "validation Loss: 0.0000 Acc: 1.0000\n",
            "\n",
            "Epoch 2/4\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 1.0000\n",
            "validation Loss: 0.0000 Acc: 1.0000\n",
            "\n",
            "Epoch 3/4\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 1.0000\n",
            "validation Loss: 0.0000 Acc: 1.0000\n",
            "\n",
            "Epoch 4/4\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 1.0000\n",
            "validation Loss: 0.0000 Acc: 1.0000\n",
            "\n",
            "Training complete in 7m 23s\n",
            "Best val Acc: 1.000000\n",
            "Accuracy for Class 0: 1.0000\n",
            "Accuracy for Class 1: 0.0000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# from tempfile import TemporaryDirectory\n",
        "import time\n",
        "\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=3):\n",
        "    since = time.time()\n",
        "\n",
        "    # Create a temporary directory to save training checkpoints\n",
        "    # with TemporaryDirectory() as tempdir:\n",
        "        # best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
        "\n",
        "        # torch.save(model.state_dict(), best_model_params_path)\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'validation']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "                class_correct = [0.0, 0.0]  # Accumulate correct predictions for each class\n",
        "                class_total = [0, 0]    # Accumulate total count of images for each class\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                    else:\n",
        "                        # Calculate accuracy for each class\n",
        "                        correct = (preds == labels).squeeze()\n",
        "                        for i in range(len(labels)):\n",
        "                            label = labels[i]\n",
        "                            class_correct[label] += correct[i].item()\n",
        "                            class_total[label] += 1\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            if phase == 'validation' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                # deep copy the model\n",
        "                # torch.save(model.state_dict(), best_model_params_path)\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Acc: {best_acc:4f}')\n",
        "    # Print accuracy for each class\n",
        "    for i in range(2):  # Assuming two classes (0 and 1)\n",
        "        accuracy = class_correct[i] / class_total[i] if class_total[i] != 0 else 0\n",
        "        print(f'Accuracy for Class {i}: {accuracy:.4f}')\n",
        "\n",
        "    # load best model weights\n",
        "    # model.load_state_dict(torch.load(best_model_params_path))\n",
        "    return model\n",
        "\n",
        "train_model(model, criterion, optimizer, scheduler, num_epochs=5)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
